{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMZWk4R7WFI3mJ9c9PLgxXV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Machine Learning Challenge Extended**\n","\n","This is an extension of my Machine Learning Challenge.\n","\n","This time I wanted to experiment using Bounding boxes, which are annotation markers drawn around the objects in an image."],"metadata":{"id":"jXIzPi1rhfZ4"}},{"cell_type":"markdown","source":["### **Step 1: Prequisities**"],"metadata":{"id":"ndEl6nPfkBpu"}},{"cell_type":"markdown","source":["**Install the YOLOv8 pre-trained model**"],"metadata":{"id":"m7l1NeLxkUji"}},{"cell_type":"code","source":["#Install YOLOv8 from pip. Ultralytics YOLO comes with a pythonic Model and Trainer interface\n","!pip install ultralytics &> /dev/null\n","\n","#To use YOLOv8 on Python and display the result, you will need the following libraries:\n","from ultralytics import YOLO\n","import numpy as np\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","import cv2\n","\n","#And if you are on Google Colab also import this one:\n","from google.colab.patches import cv2_imshow"],"metadata":{"id":"2I1oSTmSkQcD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load a pre-trained version of YOLOv8 (by default ultralytics gives us the most recent one):\n","model = YOLO(\"yolov8n.pt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JPsw10yJld8p","executionInfo":{"status":"ok","timestamp":1691839863974,"user_tz":-60,"elapsed":332,"user":{"displayName":"T M","userId":"15367820584161160438"}},"outputId":"72f1d733-5677-4d13-d563-d39ed748bb61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n","100%|██████████| 6.23M/6.23M [00:00<00:00, 275MB/s]\n"]}]},{"cell_type":"markdown","source":["**Load an image, transform it into a numpy array, then run prediction.**\n","\n","In this example, copy and paste the following URL when prompted: https://images.unsplash.com/photo-1600880292203-757bb62b4baf?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2070&q=80"],"metadata":{"id":"B3S-zugFs3KC"}},{"cell_type":"code","source":["#Loading an image from the internet as URL ot from directory path:\n","image_path = str(input(\"Image's URL or directory path:\")).strip() #strip remove spaces at the beginning and at the end of the string\n","\n","#Transforming the image into a numpy array:\n","response = requests.get(image_path)\n","image = Image.open(BytesIO(response.content))\n","image = np.asarray(image)\n","\n","#Run the prediction on our image (this should be fast, even without GPU):\n","if 'https://' in image_path or 'http://' in image_path:\n","  results = model.predict(image) #to run prediction on image with http prefix\n","else:\n","  results = model.predict(source='{}'.format(image_path)) #to run prediction on image from file directory\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PxzrOiQqw68","executionInfo":{"status":"ok","timestamp":1691839913482,"user_tz":-60,"elapsed":41052,"user":{"displayName":"T M","userId":"15367820584161160438"}},"outputId":"a8971e9c-31c8-4a59-ccb0-7c3ec6a6cb75"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Image's URL or directory path:https://images.unsplash.com/photo-1600880292203-757bb62b4baf?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2070&q=80\n"]},{"output_type":"stream","name":"stderr","text":["\n","0: 448x640 2 persons, 3 cups, 2 chairs, 1 dining table, 1 laptop, 306.7ms\n","Speed: 22.8ms preprocess, 306.7ms inference, 32.1ms postprocess per image at shape (1, 3, 448, 640)\n"]}]},{"cell_type":"markdown","source":["### **Step 2: Display the bounding box data for the first detected object**"],"metadata":{"id":"pRSVIY0Ol1x_"}},{"cell_type":"code","source":["print(results[0].boxes.data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"exh9nXoCuR1J","executionInfo":{"status":"ok","timestamp":1691841215065,"user_tz":-60,"elapsed":210,"user":{"displayName":"T M","userId":"15367820584161160438"}},"outputId":"5a56d298-e8f5-48bd-a705-5822ae0c2f52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.9521e+02, 1.7308e+02, 1.0572e+03, 1.0485e+03, 9.3203e-01, 0.0000e+00],\n","        [9.0549e+02, 3.1001e+02, 1.6536e+03, 1.0160e+03, 8.9251e-01, 0.0000e+00]])\n"]}]},{"cell_type":"markdown","source":["Comment: Here I have retreieved the bounding box data for the first detected object. This is useful for visualizing and verifying the accuracy of object detection algorithms.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"b-er3fJNlCQN"}},{"cell_type":"markdown","source":["### **Step 3: Visually display the bounding box data for objects**"],"metadata":{"id":"8KCC6NLPmp0W"}},{"cell_type":"markdown","source":["** Define the functions that will display the bounding boxes with the label and the score**"],"metadata":{"id":"CDWSC-PE5vJk"}},{"cell_type":"code","source":["def box_label(image, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255)):\n","  lw = max(round(sum(image.shape) / 2 * 0.003), 2)\n","  p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n","  cv2.rectangle(image, p1, p2, color, thickness=lw, lineType=cv2.LINE_AA)\n","  if label:\n","    tf = max(lw - 1, 1)  # font thickness\n","    w, h = cv2.getTextSize(label, 0, fontScale=lw / 3, thickness=tf)[0]  # text width, height\n","    outside = p1[1] - h >= 3\n","    p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n","    cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)  # filled\n","    cv2.putText(image,\n","                label, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n","                0,\n","                lw / 3,\n","                txt_color,\n","                thickness=tf,\n","                lineType=cv2.LINE_AA)\n","\n","def plot_bboxes(image, boxes, labels=[], colors=[], score=True, conf=None):\n","  #Define COCO Labels\n","  if labels == []:\n","    labels = {0: u'__background__', 1: u'person', 2: u'bicycle',3: u'car', 4: u'motorcycle', 5: u'airplane', 6: u'bus', 7: u'train', 8: u'truck', 9: u'boat', 10: u'traffic light', 11: u'fire hydrant', 12: u'stop sign', 13: u'parking meter', 14: u'bench', 15: u'bird', 16: u'cat', 17: u'dog', 18: u'horse', 19: u'sheep', 20: u'cow', 21: u'elephant', 22: u'bear', 23: u'zebra', 24: u'giraffe', 25: u'backpack', 26: u'umbrella', 27: u'handbag', 28: u'tie', 29: u'suitcase', 30: u'frisbee', 31: u'skis', 32: u'snowboard', 33: u'sports ball', 34: u'kite', 35: u'baseball bat', 36: u'baseball glove', 37: u'skateboard', 38: u'surfboard', 39: u'tennis racket', 40: u'bottle', 41: u'wine glass', 42: u'cup', 43: u'fork', 44: u'knife', 45: u'spoon', 46: u'bowl', 47: u'banana', 48: u'apple', 49: u'sandwich', 50: u'orange', 51: u'broccoli', 52: u'carrot', 53: u'hot dog', 54: u'pizza', 55: u'donut', 56: u'cake', 57: u'chair', 58: u'couch', 59: u'potted plant', 60: u'bed', 61: u'dining table', 62: u'toilet', 63: u'tv', 64: u'laptop', 65: u'mouse', 66: u'remote', 67: u'keyboard', 68: u'cell phone', 69: u'microwave', 70: u'oven', 71: u'toaster', 72: u'sink', 73: u'refrigerator', 74: u'book', 75: u'clock', 76: u'vase', 77: u'scissors', 78: u'teddy bear', 79: u'hair drier', 80: u'toothbrush'}\n","  #Define colors\n","  if colors == []:\n","    #colors = [(6, 112, 83), (253, 246, 160), (40, 132, 70), (205, 97, 162), (149, 196, 30), (106, 19, 161), (127, 175, 225), (115, 133, 176), (83, 156, 8), (182, 29, 77), (180, 11, 251), (31, 12, 123), (23, 6, 115), (167, 34, 31), (176, 216, 69), (110, 229, 222), (72, 183, 159), (90, 168, 209), (195, 4, 209), (135, 236, 21), (62, 209, 199), (87, 1, 70), (75, 40, 168), (121, 90, 126), (11, 86, 86), (40, 218, 53), (234, 76, 20), (129, 174, 192), (13, 18, 254), (45, 183, 149), (77, 234, 120), (182, 83, 207), (172, 138, 252), (201, 7, 159), (147, 240, 17), (134, 19, 233), (202, 61, 206), (177, 253, 26), (10, 139, 17), (130, 148, 106), (174, 197, 128), (106, 59, 168), (124, 180, 83), (78, 169, 4), (26, 79, 176), (185, 149, 150), (165, 253, 206), (220, 87, 0), (72, 22, 226), (64, 174, 4), (245, 131, 96), (35, 217, 142), (89, 86, 32), (80, 56, 196), (222, 136, 159), (145, 6, 219), (143, 132, 162), (175, 97, 221), (72, 3, 79), (196, 184, 237), (18, 210, 116), (8, 185, 81), (99, 181, 254), (9, 127, 123), (140, 94, 215), (39, 229, 121), (230, 51, 96), (84, 225, 33), (218, 202, 139), (129, 223, 182), (167, 46, 157), (15, 252, 5), (128, 103, 203), (197, 223, 199), (19, 238, 181), (64, 142, 167), (12, 203, 242), (69, 21, 41), (177, 184, 2), (35, 97, 56), (241, 22, 161)]\n","    colors = [(89, 161, 197),(67, 161, 255),(19, 222, 24),(186, 55, 2),(167, 146, 11),(190, 76, 98),(130, 172, 179),(115, 209, 128),(204, 79, 135),(136, 126, 185),(209, 213, 45),(44, 52, 10),(101, 158, 121),(179, 124, 12),(25, 33, 189),(45, 115, 11),(73, 197, 184),(62, 225, 221),(32, 46, 52),(20, 165, 16),(54, 15, 57),(12, 150, 9),(10, 46, 99),(94, 89, 46),(48, 37, 106),(42, 10, 96),(7, 164, 128),(98, 213, 120),(40, 5, 219),(54, 25, 150),(251, 74, 172),(0, 236, 196),(21, 104, 190),(226, 74, 232),(120, 67, 25),(191, 106, 197),(8, 15, 134),(21, 2, 1),(142, 63, 109),(133, 148, 146),(187, 77, 253),(155, 22, 122),(218, 130, 77),(164, 102, 79),(43, 152, 125),(185, 124, 151),(95, 159, 238),(128, 89, 85),(228, 6, 60),(6, 41, 210),(11, 1, 133),(30, 96, 58),(230, 136, 109),(126, 45, 174),(164, 63, 165),(32, 111, 29),(232, 40, 70),(55, 31, 198),(148, 211, 129),(10, 186, 211),(181, 201, 94),(55, 35, 92),(129, 140, 233),(70, 250, 116),(61, 209, 152),(216, 21, 138),(100, 0, 176),(3, 42, 70),(151, 13, 44),(216, 102, 88),(125, 216, 93),(171, 236, 47),(253, 127, 103),(205, 137, 244),(193, 137, 224),(36, 152, 214),(17, 50, 238),(154, 165, 67),(114, 129, 60),(119, 24, 48),(73, 8, 110)]\n","\n","  #plot each boxes\n","  for box in boxes:\n","    #add score in label if score=True\n","    if score :\n","      label = labels[int(box[-1])+1] + \" \" + str(round(100 * float(box[-2]),1)) + \"%\"\n","    else :\n","      label = labels[int(box[-1])+1]\n","    #filter every box under conf threshold if conf threshold setted\n","    if conf :\n","      if box[-2] > conf:\n","        color = colors[int(box[-1])]\n","        box_label(image, box, label, color)\n","    else:\n","      color = colors[int(box[-1])]\n","      box_label(image, box, label, color)\n","\n","  try:\n","    import google.colab\n","    IN_COLAB = True\n","  except:\n","    IN_COLAB = False\n","\n","  if IN_COLAB:\n","    cv2_imshow(image) #if used in Colab\n","  else :\n","    plt.figure(figsize=(20,10))\n","    plt.imshow(image) #if used in Python"],"metadata":{"id":"99RqNsHFvRzY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Import the necessary module(s) needed**"],"metadata":{"id":"s13u8sDVnelY"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt #for visualuation\n","plot_bboxes(image, results[0].boxes.boxes, score=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453,"output_embedded_package_id":"1d_u4IiTMFQBfAU5jtIa_hiWuc7p-P2TQ"},"id":"5eptF-Po7BqY","executionInfo":{"status":"ok","timestamp":1691839978986,"user_tz":-60,"elapsed":3000,"user":{"displayName":"T M","userId":"15367820584161160438"}},"outputId":"cdbc9d4b-ba8e-44a0-d06b-9eacbe6146d5"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Comment:\n","I get a lot of objects. Some are relevant, and some are irrelevant - like the chairs.\n","\n","The YOLOv8n.pt (pre-trained) model is able to detect 80 classes, which includes some of the required objects. But the model did not detect all these objects with the same confidence score.\n","\n","Solution: i can filter these objects according to this score. For example, I can display only the bounding boxes with a confidence score higher than 85%. Thus, all the objects detected with a lower score will not be displayed."],"metadata":{"id":"wCM5K_rDyrHW"}},{"cell_type":"markdown","source":["**To filter according to the confidence score, I indicate conf=0.85:**"],"metadata":{"id":"WcBfuYbwn8Dl"}},{"cell_type":"code","source":["response = requests.get(image_path)\n","image = Image.open(BytesIO(response.content))\n","image = np.asarray(image)\n","\n","#Run the prediction on our image where conf=0.85:\n","if 'https://' in image_path or 'http://' in image_path:\n","  results = model.predict(image, conf = 0.85) #to run prediction on image with http prefix\n","else:\n","  results = model.predict(source='{}'.format(image_path), conf = 0.85) #to run prediction on image from file directory\n"],"metadata":{"id":"cdiiwqiFDMG0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691839988006,"user_tz":-60,"elapsed":1408,"user":{"displayName":"T M","userId":"15367820584161160438"}},"outputId":"efcf5049-5c70-4e37-e214-1c4b1ed43216"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","0: 448x640 2 persons, 128.9ms\n","Speed: 4.1ms preprocess, 128.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n"]}]},{"cell_type":"code","source":["image = Image.open(BytesIO(response.content))\n","image = np.asarray(image)\n","\n","plot_bboxes(image, results[0].boxes.boxes, score=False, conf=0.85)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453,"output_embedded_package_id":"1Nbbbkf9Dlk6dQV7tFvhu7erPXBmOyioh"},"id":"-1tClnN53rum","executionInfo":{"status":"ok","timestamp":1691839994755,"user_tz":-60,"elapsed":3345,"user":{"displayName":"T M","userId":"15367820584161160438"}},"outputId":"3f1a08ee-238d-4e83-db23-aafdbde7ba72"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["With a confidence threshold of 0.85, YOLOv8n was able to detect 2 persons."],"metadata":{"id":"OkvHFN9d0I7N"}}]}